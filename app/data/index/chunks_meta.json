[
  {
    "source": "DataMining-Unit-1.pdf",
    "text": " \n \nUNIT I \nDATA MINING \nIntroduction – Data – Types of Data – Data Mining Functionalities – Interestingness of \nPatterns – Classification of Data Mining Systems – Data Mining Task Primitives – \nIntegration of a Data Mining System with a Data Warehouse – Major Issues in Data \nMining –Data Preprocessing. \n \nData \n \n Collection of data objects and their attributes \n An attribute is a property or characteristic of an object \n Examples: eye color of a person, temperature, etc. \n Attribute is also known as variable, field, characteristic, or feature \n A collection of attributes describe an object \n Object is also known as record, point, case, sample, entity, or instance Attributes \n \nObjects Attribute Values \n \nAttribute values are numbers or symbols assigned to an attribute \n \nDistinction between attributes and attribute values \n \nSame attribute can be mapped to different attribute values \n \nExample: height can be measured in feet or meters \n \nDifferent attributes can be mapped "
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "ion between attributes and attribute values \n \nSame attribute can be mapped to different attribute values \n \nExample: height can be measured in feet or meters \n \nDifferent attributes can be mapped to the same set of values \n \nExample: Attribute values for ID and age are integers \n \nBut properties of attribute values can be different \n \nID has no limit but age has a maximum and minimum value \n \nTypes of Attributes \nThis is the First step of Data-preprocessing. We differentiate between different types of \nattributes and then preprocess the data. So here is the description of attribute types.  \n1. \nQualitative (Nominal (N), Ordinal (O), Binary(B)).  \n2. \n Quantitative (Numeric, Discrete, Continuous)  \n\n \n \n  \n \n \n \nQualitative  Attributes: \n  \n1. Nominal Attributes – related to names: The values of a Nominal attribute are names of \nthings, some kind of symbols. Values of Nominal attributes represents some category or state \nand that’s why nominal attribute also referred as categoric"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "he values of a Nominal attribute are names of \nthings, some kind of symbols. Values of Nominal attributes represents some category or state \nand that’s why nominal attribute also referred as categorical attributes and there is no order \n(rank, \nposition) \namong \nvalues \nof \nthe \nnominal \nattribute.  \nExample :  \n \n \nBinary Attributes: Binary data has only 2 values/states. For Example yes or no, affected or \nunaffected, true or false \n.  \n \nSymmetric: Both values are equally important (Gender).  \n \n \n \nAsymmetric: Both values are not equally important (Result). \n \n\n \n \n \n \nOrdinal Attributes : The Ordinal Attributes contains values that have a meaningful \nsequence or ranking(order) between them, but the magnitude between values is not actually \nknown, the order of values that shows what is important but don’t indicate how important it \nis.  \n \n \n \nQuantitative Attributes: \n  \n1. Numeric: A numeric attribute is quantitative because, it is a measurable quantity, \nrepresented in integer "
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "s important but don’t indicate how important it \nis.  \n \n \n \nQuantitative Attributes: \n  \n1. Numeric: A numeric attribute is quantitative because, it is a measurable quantity, \nrepresented in integer or real values. Numerical attributes are of 2 types, interval, \nand ratio.  \n \nAn interval-scaled attribute has values, whose differences are interpretable, but the \nnumerical attributes do not have the correct reference point, or we can call zero points. \nData can be added and subtracted at an interval scale but can not be multiplied or divided. \nConsider an example of temperature in degrees Centigrade. If a day’s temperature of one \nday is twice of the other day we cannot say that one day is twice as hot as another day.  \n \nA ratio-scaled attribute is a numeric attribute with a fix zero-point. If a \nmeasurement is ratio-scaled, we can say of a value as being a multiple (or ratio) of another \nvalue. The values are ordered, and we can also compute the difference between values, \nand the "
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "int. If a \nmeasurement is ratio-scaled, we can say of a value as being a multiple (or ratio) of another \nvalue. The values are ordered, and we can also compute the difference between values, \nand the mean, median, mode, Quantile-range, and Five \n number \n summary \n \ncan  be \n given. \n  \n \n\n \n \n2. Discrete : Discrete data have finite values it can be numerical and can also be in \ncategorical \nform. \nThese \nattributes \nhas \nfinite \nset \nof \nvalues.  \n  \nExample: \n  \n \n \n3. Continuous: Continuous data have an infinite no of states. Continuous data is of float type. \nThere can be many values between 2 and 3.  \nExample :  \n \n \n \nTypes of Data \n \n \n \n \nFlat files: Flat files are actually the most common data source for data mining algorithms, \nespecially at the research level. Flat files are simple data files in text or binary format with a \n\n \n \nstructure known by the data mining algorithm to be applied. The data in these files can be \ntransactions, time-series data, scientific measurements"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "ple data files in text or binary format with a \n\n \n \nstructure known by the data mining algorithm to be applied. The data in these files can be \ntransactions, time-series data, scientific measurements, etc. \nRelational Databases: a relational database consists of a set of tables containing either \nvalues of entity attributes, or values of attributes from entity relationships. Tables have \ncolumns and rows, where columns represent attributes and rows represent tuples. A tuple in \na relational table corresponds to either an object or a relationship between objects and is \nidentified by a set of attribute values representing a unique key. In following figure it \npresents some relations Customer, Items, and Borrow representing business activity in a \nvideo store. These relations are just a subset of what could be a database for the video store \nand is given as an example. \n \n \n \nThe most commonly used query language for relational database is SQL, which allows \nretrieval and manipulation o"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "set of what could be a database for the video store \nand is given as an example. \n \n \n \nThe most commonly used query language for relational database is SQL, which allows \nretrieval and manipulation of the data stored in the tables, as well as the calculation of \naggregate functions such as average, sum, min, max and count. For instance, an SQL query \nto select the videos grouped by category would be: \nSELECT count(*) FROM Items WHERE type=video GROUP BY category. \nData mining algorithms using relational databases can be more versatile than data mining \nalgorithms specifically written for flat files, since they can take advantage of the structure \ninherent to relational databases. While data mining can benefit from SQL for data selection, \ntransformation and consolidation, it goes beyond what SQL could provide, such as \npredicting, comparing, detecting deviations, etc. \n\n \n \nTransactional databases \nIn general, a transactional database consists of a flat file where each record represen"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "hat SQL could provide, such as \npredicting, comparing, detecting deviations, etc. \n\n \n \nTransactional databases \nIn general, a transactional database consists of a flat file where each record represents a \ntransaction. A transaction typically includes a unique transaction identity number (trans ID), \nand a list of the items making up the transaction (such as items purchased in a store) as \nshown below: \n \n       SALES \nTrans-ID \nList of item_ID’s \nT100 \n…….. \nI1,I3,I8 \n……… \nA spatial database contains spatial-related data, which may be represented in the form of \nraster or vector data. Raster data consists of n-dimensional bit maps or pixel maps, and \nvector data are represented by lines, points, polygons or other kinds of processed primitives, \nSome examples of spatial databases include geographical (map) databases, VLSI chip \ndesigns, and medical and satellite images databases. \nData warehouses \nA data warehouse is a repository of information collected from multiple sources, stored u"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "eographical (map) databases, VLSI chip \ndesigns, and medical and satellite images databases. \nData warehouses \nA data warehouse is a repository of information collected from multiple sources, stored under a \nunified schema, and which usually resides at a single site. Data warehouses are constructed via a \nprocess of data cleansing, data transformation, data integration, data loading, and periodic data \nrefreshing. The figure shows the basic architecture of a data warehouse. \nIn order to facilitate decision making, the data in a data warehouse are organized around major \nsubjects, such as customer, item, supplier, and activity. The data are stored to provide \ninformation from a historical perspective and are typically summarized. \n\n \n \nA data warehouse is usually modeled by a multidimensional database structure, where each \ndimension corresponds to an attribute or a set of attributes in the schema, and each cell stores \nthe value of some aggregate measure, such as count or sales amount."
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "al database structure, where each \ndimension corresponds to an attribute or a set of attributes in the schema, and each cell stores \nthe value of some aggregate measure, such as count or sales amount. The actual physical \nstructure of a data warehouse may be a relational data store or a multidimensional data cube. It \nprovides a multidimensional view of data and allows the precomputation and fast accessing of \nsummarized \n data. \nThe data cube structure that stores the primitive or lowest level of information is called a base \ncuboid. Its corresponding higher level multidimensional (cube) structures are called (non-base) \ncuboids. A base cuboid together with all of its corresponding higher level cuboids form a data \ncube. By providing multidimensional data views and the precomputation of summarized data, \ndata warehouse systems are well suited for On- Line Analytical Processing, or OLAP. OLAP \noperations make use of background knowledge regarding the domain of the data being studied \ni"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "ummarized data, \ndata warehouse systems are well suited for On- Line Analytical Processing, or OLAP. OLAP \noperations make use of background knowledge regarding the domain of the data being studied \nin order to allow the presentation of data at different levels of abstraction. Such operations \naccommodate different user viewpoints. Examples of OLAP operations include drill-down and \nroll-up, which allow the user to view the data at differing degrees of summarization, as \nillustrated in above figure. \n \n \n \n\n \n \n \nA multimedia database stores images, audio, and video data, and is used in applications such as \npicture content-based retrieval, voice-mail systems, video-on- demand systems, the World Wide \nWeb, and speech-based user interfaces. \nTime-Series Databases: Time-series databases contain time related data such stock market data or \nlogged activities. These databases usually have a continuous flow of new data coming in, which \nsometimes causes the need for a challenging real time a"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "ain time related data such stock market data or \nlogged activities. These databases usually have a continuous flow of new data coming in, which \nsometimes causes the need for a challenging real time analysis. Data mining in such databases \ncommonly includes the study of trends and correlations between evolutions of different variables, as \nwell as the prediction of trends and movements of the variables in time. \nThe World-Wide Web provides rich, world-wide, on-line information services, where data objects \nare linked together to facilitate interactive access. Some examples of distributed information \nservices associated with the World-Wide Web include America Online, Yahoo!, AltaVista, and \nProdigy. \n \nData mining \n \nData mining is one of the most useful techniques that help entrepreneurs, researchers, and  \nindividuals to extract valuable information from huge sets of data. Data mining is also \ncalled Knowledge Discovery in Database (KDD). The knowledge discovery process includes Data"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": " researchers, and  \nindividuals to extract valuable information from huge sets of data. Data mining is also \ncalled Knowledge Discovery in Database (KDD). The knowledge discovery process includes Data \ncleaning, Data integration, Data selection, Data transformation, Data mining, Pattern evaluation, and \nKnowledge presentation. \n \n\n \n \nMany people treat data mining as a synonym for another popularly used term,knowledge discovery \nfrom data, or KDD, while others view data mining as merely an essential step in the process of \nknowledge discovery. The knowledge discovery process is shown in Figure  as an iterative sequence \nof the following steps: \n \n1. Data cleaning (to remove noise and inconsistent data) \n2. Data integration (where multiple data sources may be combined) \n3. Data selection (where data relevant to the analysis task are retrieved from the \ndatabase) \n4. Data transformation (where data are transformed and consolidated into forms \nappropriate for mining by performing summary "
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "here data relevant to the analysis task are retrieved from the \ndatabase) \n4. Data transformation (where data are transformed and consolidated into forms \nappropriate for mining by performing summary or aggregation operations) \n5. Data mining (an essential process where intelligent methods are applied to extract \ndata patterns) \n6. Pattern evaluation (to identify the truly interesting patterns representing knowledge \nbased on interestingness measures \n7. Knowledge presentation (where visualization and knowledge representation techniques \nare used to present mined knowledge to users) \nSteps 1 through 4 are different forms of data preprocessing, where data are prepared for mining. The \ndata mining step may interact with the user or a knowledge base. The interesting patterns are \npresented to the user and may be stored as new knowledge in the knowledge base. \n\n \n \nThe preceding view shows data mining as one step in the knowledge discovery process,albeit an \nessential one because it uncove"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": " the user and may be stored as new knowledge in the knowledge base. \n\n \n \nThe preceding view shows data mining as one step in the knowledge discovery process,albeit an \nessential one because it uncovers hidden patterns for evaluation. However,in industry, in media, and \nin the research milieu, the term data mining is often used to refer to the entire knowledge discovery \nprocess (perhaps because the term is shorter than knowledge discovery from data). Therefore, we \nadopt a broad view of data mining functionality: Data mining is the process of discovering \ninteresting patterns and knowledge from large amounts of data. The data sources can include \ndatabases, data warehouses, the Web, other information repositories, or data that are streamed into \nthe system dynamically \nArchitecture of a typical data mining system. \n \nThe architecture of a typical data mining system may have the following major components \n \n1. \nDatabase, data warehouse, or other information repository. This is one or "
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "ypical data mining system. \n \nThe architecture of a typical data mining system may have the following major components \n \n1. \nDatabase, data warehouse, or other information repository. This is one or a set of databases, \ndata warehouses, spread sheets, or other kinds of information repositories. Data cleaning and data \nintegration techniques may be performed on the data. \n2. \nDatabase or data warehouse server. The database or data warehouse server is \nresponsible or fetching the relevant data, based on the user's data mining request. \n3. \nKnowledge base. This is the domain knowledge that is used to guide the search, or \nevaluate the interestingness of resulting patterns. Such knowledge can include concept hierarchies, \nused to organize attributes or attribute values into different levels of abstraction. Knowledge such as \nuser beliefs, which can be used to assess a pattern's interestingness based on its unexpectedness, \nmay also be included. \n4. \nData mining engine. This is essential t"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "f abstraction. Knowledge such as \nuser beliefs, which can be used to assess a pattern's interestingness based on its unexpectedness, \nmay also be included. \n4. \nData mining engine. This is essential to the data mining system and ideally consists of a \nset of functional modules for tasks such as characterization, association analysis, classification, \nevolution and deviation analysis. \n5. \nPattern evaluation module. This component typically employs interestingness measures \nand interacts with the data mining modules so as to focus the search towards interesting patterns. It \nmay access interestingness thresholds stored in the knowledge base. Alternatively, the pattern \nevaluation module may be integrated with the mining module, depending on the implementation of \nthe data mining method used. \n6. \nGraphical user interface. This module communicates between users and the data mining \nsystem, allowing the user to interact with the system by specifying a data mining query or task, \n\n \n \nprov"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": ". \n6. \nGraphical user interface. This module communicates between users and the data mining \nsystem, allowing the user to interact with the system by specifying a data mining query or task, \n\n \n \nproviding information to help focus the search, and performing exploratory data mining based on \nthe intermediate data mining results. \n \n \n \nFigure: Architecture of a typical data mining system \n \nData Mining Functionalities \nData mining functions are used to define the trends or correlations contained in data mining \nactivities. In comparison, data mining activities can be divided into two categories: \no Descriptive Data Mining: It includes certain knowledge to understand what is happening \nwithin the data without a previous idea. The common data features are highlighted in the data \nset. For example, count, average etc. \no Predictive Data Mining: It helps developers to provide unlabeled definitions of attributes. \nWith previously available or historical data, data mining can be used to make"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "ample, count, average etc. \no Predictive Data Mining: It helps developers to provide unlabeled definitions of attributes. \nWith previously available or historical data, data mining can be used to make predictions about \ncritical business metrics based on data's linearity. For example, predicting the volume of \nbusiness next quarter based on performance in the previous quarters over several years or \njudging from the findings of a patient's medical examinations that is he suffering from any \nparticular disease. \nData mining functionalities are used to represent the type of patterns that have to be discovered in \ndata mining tasks. Data mining is extensively used in many areas or sectors. It is used to predict and \ncharacterize data. But the ultimate objective in Data Mining Functionalities is to observe the \nvarious trends in data mining. There are several data mining functionalities that the organized and \nscientific methods offer, such as: \n\n \n \n \n \n1. Class/Concept Descriptions \nA cl"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "is to observe the \nvarious trends in data mining. There are several data mining functionalities that the organized and \nscientific methods offer, such as: \n\n \n \n \n \n1. Class/Concept Descriptions \nA class or concept implies there is a data set or set of features that define the class or a concept. \nA class can be a category of items on a shop floor, and a concept could be the abstract idea on \nwhich data may be categorized like products to be put on clearance sale and non-sale products. \nThere are two concepts here, one that helps with grouping and the other that helps in \ndifferentiating. \no Data Characterization: This refers to the summary of general characteristics or features of the \nclass, resulting in specific rules that define a target class. A data analysis technique called \nAttribute-oriented Induction is employed on the data set for achieving characterization. \no Data Discrimination: Discrimination is used to separate distinct data sets based on the disparity \nin attribute val"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "te-oriented Induction is employed on the data set for achieving characterization. \no Data Discrimination: Discrimination is used to separate distinct data sets based on the disparity \nin attribute values. It compares features of a class with features of one or more contrasting \nclasses.g., bar charts, curves and pie charts. \n2. Mining Frequent Patterns \nOne of the functions of data mining is finding data patterns. Frequent patterns are things that are \ndiscovered to be most common in data. Various types of frequency can be found in the dataset. \no Frequent item set:This term refers to a group of items that are commonly found together, such as \nmilk and sugar. \no Frequent substructure: It refers to the various types of data structures that can be combined with \nan item set or subsequences, such as trees and graphs. \no Frequent Subsequence: A regular pattern series, such as buying a phone followed by a cover. \n3. Association Analysis \n\n \n \nIt analyses the set of items that generally occu"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "such as trees and graphs. \no Frequent Subsequence: A regular pattern series, such as buying a phone followed by a cover. \n3. Association Analysis \n\n \n \nIt analyses the set of items that generally occur together in a transactional dataset. It is also known \nas Market Basket Analysis for its wide use in retail sales. Two parameters are used for determining \nthe association rules: \nSupport : It provides which identifies the common item set in the database. \nConfidence :  is the conditional probability that an item occurs when another item occurs in a \ntransaction. \n4. Classification \nClassification is a data mining technique that categorizes items in a collection based on some \npredefined properties. It uses methods like if-then, decision trees or neural networks to predict a \nclass or essentially classify a collection of items. A training set containing items whose \nproperties are known is used to train the system to predict the category of items from an \nunknown collection of items. \n5."
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "ially classify a collection of items. A training set containing items whose \nproperties are known is used to train the system to predict the category of items from an \nunknown collection of items. \n5. Prediction \nIt  predicts some unavailable data values or spending trends. An object can be anticipated based on \nthe attribute values of the object and attribute values of the classes. It can be a prediction of missing \nnumerical values or increase or decrease trends in time-related information. There are primarily two \ntypes of predictions in data mining: numeric and class predictions. \no Numeric predictions are made by creating a linear regression model that is based on historical data. \nPrediction of numeric values helps businesses ramp up for a future event that might impact the \nbusiness positively or negatively. \no Class predictions are used to fill in missing class information for products using a training data set \nwhere the class for products is known. \n6. Cluster Analysis \nIn im"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": " positively or negatively. \no Class predictions are used to fill in missing class information for products using a training data set \nwhere the class for products is known. \n6. Cluster Analysis \nIn image processing, pattern recognition and bioinformatics, clustering is a popular data mining \nfunctionality. It is similar to classification, but the classes are not predefined. Data attributes \nrepresent the classes. Similar data are grouped together, with the difference being that a class label \nis not known. Clustering algorithms group data based on similar features and dissimilarities. \n7. Outlier Analysis \n\n \n \nOutlier analysis is important to understand the quality of data. If there are too many outliers, you \ncannot trust the data or draw patterns. An outlier analysis determines if there is something out of \nturn in the data and whether it indicates a situation that a business needs to consider and take \nmeasures to mitigate. An outlier analysis of the data that cannot be grouped int"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": " is something out of \nturn in the data and whether it indicates a situation that a business needs to consider and take \nmeasures to mitigate. An outlier analysis of the data that cannot be grouped into any classes by the \nalgorithms is pulled up. \n8. Evolution and Deviation Analysis \nEvolution Analysis pertains to the study of data sets that change over time. Evolution analysis \nmodels are designed to capture evolutionary trends in data helping to characterize, classify, cluster \nor discriminate time-related data. \n9. Correlation Analysis \nCorrelation is a mathematical technique for determining whether and how strongly two attributes is \nrelated to one another. It refers to the various types of data structures, such as trees and graphs, that \ncan be combined with an item set or subsequence. It determines how well two numerically \nmeasured continuous variables are linked. Researchers can use this type of analysis to see if there \nare any possible correlations between variables in their "
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": ". It determines how well two numerically \nmeasured continuous variables are linked. Researchers can use this type of analysis to see if there \nare any possible correlations between variables in their study. \nInterestingness Patterns \nA pattern is interesting if it is (1) easily understood by humans, (2) valid on new or test data with \nsome degree of certainty, (3) potentially useful, and (4) novel. A pattern is also interesting if it \nvalidates a hypothesis that the user sought to confirm. An interesting pattern represents knowledge. \nobjective measures of pattern interestingness  are based on the structure of discovered patterns \nand the statistics underlying them. An objective measure for association rules of the form X      Y  \nis rule support, representing the percentage of transactions from a transaction database that the \ngiven rule satisfies. This is taken to be the probability P.(X U Y) where X UY indicates that a \ntransaction contains both X and Y, that is, the union of itemse"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "s from a transaction database that the \ngiven rule satisfies. This is taken to be the probability P.(X U Y) where X UY indicates that a \ntransaction contains both X and Y, that is, the union of itemsets X and Y.  \nAnother objective measure for association rules is confidence, which assesses the degree of \ncertainty of the detected association. This is taken to be the conditional probability P.(Y | X), that \nis, the probability \nthat a transaction containing X also contains Y.  \n \n\n \n \nIn general, each interestingness measure is associated with a threshold, which may be controlled \nby the user. For example, rules that do not satisfy a confidence threshold of, say, 50% can be \nconsidered uninteresting. Rules below the threshold likely reflect noise,exceptions, or minority \ncases and are probably of less value. \nOther objective interestingness measures include accuracy and coverage for classification(IF-\nTHEN) rules. In general terms, accuracy tells us the percentage of data that are corr"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "ably of less value. \nOther objective interestingness measures include accuracy and coverage for classification(IF-\nTHEN) rules. In general terms, accuracy tells us the percentage of data that are correctly \nclassified by a rule. Coverage is similar to support, in that it tells us the percentage of data to \nwhich a rule applies. \n. \nSubjective interestingness measures are based on user beliefs in the data. These measures find \npatterns interesting if the patterns are unexpected (contradicting a user’s belief) or offer strategic \ninformation on which the user can act. In the latter case, such patterns are referred to as \nactionable. For example, patterns like “a large earthquake often follows a cluster of small \nquakes” may be highly actionable if users can act on the information to save lives. Patterns that \nare expected can be interesting if they confirm a hypothesis that the user wishes to validate or \nthey resemble a user’s hunch. \n \nIt is often unrealistic and inefficient for data m"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "es. Patterns that \nare expected can be interesting if they confirm a hypothesis that the user wishes to validate or \nthey resemble a user’s hunch. \n \nIt is often unrealistic and inefficient for data mining systems to generate all possible patterns. \nInstead, user provided constraints and interestingness measures should be used to focus the \nsearch. For some mining tasks, such as association, this is often sufficient to ensure the \ncompleteness of the algorithm. Association rule mining is an example where the use of \nconstraints and interestingness measures can ensure the completeness of mining.  \n \nIt is highly desirable for data mining systems to generate only interesting patterns. This would be \nefficient for users and data mining systems because neither would have to search through the \npatterns generated to identify the truly interesting ones. Progress has been made in this direction; \nhowever, such optimization remains a challenging issue in data mining. Measures of pattern \ninter"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "\npatterns generated to identify the truly interesting ones. Progress has been made in this direction; \nhowever, such optimization remains a challenging issue in data mining. Measures of pattern \ninterestingness are essential for the efficient discovery of patterns by target users. Such measures \ncan be used after the data mining step to rank the discovered patterns according to their \ninterestingness, filtering out the uninteresting ones.More important, such measures can be used \nto guide and constrain the discovery process, improving the search efficiency by pruning away \nsubsets of the pattern space that do not satisfy prespecified interestingness constraints. \n \nClassification of Datamining Systems \n\n \n \nA data mining system can be classified according to the following criteria − \n \nDatabase Technology \n \nStatistics \n \nMachine Learning \n \nInformation Science \n \nVisualization \n \nOther Disciplines \n \nApart from these, a data mining system can also be classified based on the kind"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "tabase Technology \n \nStatistics \n \nMachine Learning \n \nInformation Science \n \nVisualization \n \nOther Disciplines \n \nApart from these, a data mining system can also be classified based on the kind of  \n(a) databases mined, (b) knowledge mined, (c) techniques utilized, and (d) applications adapted. \nClassification Based on the Databases Mined \nWe can classify a data mining system according to the kind of databases mined. Database system \ncan be classified according to different criteria such as data models, types of data, etc. And the \ndata mining system can be classified accordingly.For example, if we classify a database \naccording to the data model, then we may have a relational, transactional, object-relational, or \ndata warehouse mining system. \nClassification Based on the kind of Knowledge Mined \nWe can classify a data mining system according to the kind of knowledge mined. It means the \ndata mining system is classified on the basis of functionalities such as − \n \nCharacteriza"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "nowledge Mined \nWe can classify a data mining system according to the kind of knowledge mined. It means the \ndata mining system is classified on the basis of functionalities such as − \n \nCharacterization \n \nDiscrimination \n \nAssociation and Correlation Analysis \n \nClassification \n \nPrediction \n \nOutlier Analysis \n\n \n \n \nEvolution Analysis \n \nClassification Based on the Techniques Utilized \nWe can classify a data mining system according to the kind of techniques used. We can describe \nthese techniques according to the degree of user interaction involved or the methods of analysis \nemployed. \nClassification Based on the Applications Adapted \nWe can classify a data mining system according to the applications adapted. These applications \nare as follows − \n \nFinance \n \nTelecommunications \n \nDNA \n \nStock Markets \n \nE-mail \n \nDataMining Task Primitives \nA data mining task can be specified in the form of a data mining query, which is input to the data \nmining system. A data mining "
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "ns \n \nDNA \n \nStock Markets \n \nE-mail \n \nDataMining Task Primitives \nA data mining task can be specified in the form of a data mining query, which is input to the data \nmining system. A data mining query is defined in terms of data mining task primitives. These \nprimitives allow the user to interactively communicate with the data mining system during \ndiscovery to direct the mining process or examine the findings from different angles or depths. \nThe data mining primitives specify the following, \n1. \nSet of task-relevant data to be mined. \n2. \nKind of knowledge to be mined. \n3. \nBackground knowledge to be used in the discovery process. \n4. \nInterestingness measures and thresholds for pattern evaluation. \n5. \nRepresentation for visualizing the discovered patterns. \n1. The set of task-relevant data to be mined \nThis specifies the portions of the database or the set of data in which the user is interested. This \nincludes the database attributes or data warehouse dimensions of interest ("
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "vant data to be mined \nThis specifies the portions of the database or the set of data in which the user is interested. This \nincludes the database attributes or data warehouse dimensions of interest (the relevant attributes \nor dimensions).In a relational database, the set of task-relevant data can be collected via a \nrelational query involving operations like selection, projection, join, and aggregation.The data \n\n \n \ncollection process results in a new data relational called the initial data relation. The initial data \nrelation can be ordered or grouped according to the conditions specified in the query. This data \nretrieval can be thought of as a subtask of the data mining task. This initial relation may or may \nnot correspond to physical relation in the database. Since virtual relations are called Views in the \nfield of databases, the set of task-relevant data for data mining is called a minable view. \n2. The kind of knowledge to be mined \nThis specifies the data mining functions t"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "s are called Views in the \nfield of databases, the set of task-relevant data for data mining is called a minable view. \n2. The kind of knowledge to be mined \nThis specifies the data mining functions to be performed, such as characterization, \ndiscrimination, association or correlation analysis, classification, prediction, clustering, outlier \nanalysis, or evolution analysis. \n3. The background knowledge to be used in the discovery process \nThis knowledge about the domain to be mined is useful for guiding the knowledge discovery \nprocess and evaluating the patterns found. Concept hierarchies are a popular form of background \nknowledge, which allows data to be mined at multiple levels of abstraction. Concept hierarchy \ndefines a sequence of mappings from low-level concepts to higher-level, more general concepts. \no Rolling Up - Generalization of data: Allow to view data at more meaningful and explicit \nabstractions and makes it easier to understand. It compresses the data, and it would r"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "ore general concepts. \no Rolling Up - Generalization of data: Allow to view data at more meaningful and explicit \nabstractions and makes it easier to understand. It compresses the data, and it would require fewer \ninput/output operations. \no Drilling Down - Specialization of data: Concept values replaced by lower-level concepts. \nBased on different user viewpoints, there may be more than one concept hierarchy for a given \nattribute or dimension. \nAn example of a concept hierarchy for the attribute (or dimension) age is shown below. User \nbeliefs regarding relationships in the data are another form of background knowledge. \n4. The interestingness measures and thresholds for pattern evaluation \nDifferent kinds of knowledge may have different interesting measures. They may be used to \nguide the mining process or, after discovery, to evaluate the discovered patterns. For example, \ninteresting measures for association rules include support and confidence. Rules whose support \nand confidence"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "he mining process or, after discovery, to evaluate the discovered patterns. For example, \ninteresting measures for association rules include support and confidence. Rules whose support \nand confidence values are below user-specified thresholds are considered uninteresting. \n\n \n \no Simplicity: A factor contributing to the interestingness of a pattern is the pattern's overall \nsimplicity for human comprehension. For example, the more complex the structure of a rule is, \nthe more difficult it is to interpret, and hence, the less interesting it is likely to be. Objective \nmeasures of pattern simplicity can be viewed as functions of the pattern structure, defined in \nterms of the pattern size in bits or the number of attributes or operators appearing in the pattern. \no Certainty (Confidence): Each discovered pattern should have a measure of certainty \nassociated with it that assesses the validity or \"trustworthiness\" of the pattern. A certainty \nmeasure for association rules of the form \"A "
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "ach discovered pattern should have a measure of certainty \nassociated with it that assesses the validity or \"trustworthiness\" of the pattern. A certainty \nmeasure for association rules of the form \"A =>B\" where A and B are sets of items is \nconfidence. Confidence is a certainty measure. Given a set of task-relevant data tuples, the \nconfidence \nof \n\"A=>B\" \nis \ndefined \nas \nConfidence (A=>B) = # tuples containing both A and B /# tuples containing A \no Utility (Support): The potential usefulness of a pattern is a factor defining its interestingness. \nIt can be estimated by a utility function, such as support. The support of an association pattern \nrefers to the percentage of task-relevant data tuples (or transactions) for which the pattern is true. \nUtility \n(support): \nusefulness \nof \na \npattern \nSupport (A=>B) = # tuples containing both A and B / total #of tuples \no Novelty: Novel patterns are those that contribute new information or increased performance to \nthe given pattern set. For"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "ttern \nSupport (A=>B) = # tuples containing both A and B / total #of tuples \no Novelty: Novel patterns are those that contribute new information or increased performance to \nthe given pattern set. For example -> A data exception. Another strategy for detecting novelty is \nto remove redundant patterns. \n5. The expected representation for visualizing the discovered patterns \nThis refers to the form in which discovered patterns are to be displayed, which may include \nrules, \ntables, \ncross tabs, \ncharts, \ngraphs, \ndecision \ntrees, \ncubes, or other \nvisual \nrepresentations.Users must be able to specify the forms of presentation to be used for displaying \nthe discovered patterns. Some representation forms may be better suited than others for \nparticular kinds of knowledge.For example, generalized relations and their corresponding cross \ntabs or pie/bar charts are good for presenting characteristic descriptions, whereas decision trees \nare common for classification. \n\n \n \n \nIntegration of  D"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "ed relations and their corresponding cross \ntabs or pie/bar charts are good for presenting characteristic descriptions, whereas decision trees \nare common for classification. \n\n \n \n \nIntegration of  Data Mining System with a Data warehouse \n \nIf a data mining system is not integrated with a database or a data warehouse system, then there \nwill be no system to communicate with. This scheme is known as the non-coupling scheme. In \nthis scheme, the main focus is on data mining design and on developing efficient and effective \nalgorithms for mining the available data sets. \nThe list of Integration Schemes is as follows − \n No Coupling − In this scheme, the data mining system does not utilize any of the database or data \nwarehouse functions. It fetches the data from a particular source and processes that data using \nsome data mining algorithms. The data mining result is stored in another file. \n Loose Coupling − In this scheme, the data mining system may use some of the functions of \ndata"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "cesses that data using \nsome data mining algorithms. The data mining result is stored in another file. \n Loose Coupling − In this scheme, the data mining system may use some of the functions of \ndatabase and data warehouse system. It fetches the data from the data respiratory managed by \nthese systems and performs data mining on that data. It then stores the mining result either in a \nfile or in a designated place in a database or in a data warehouse. \n Semi−tight Coupling − In this scheme, the data mining system is linked with a database or a data \nwarehouse system and in addition to that, efficient implementations of a few data mining \nprimitives can be provided in the database. \n Tight coupling − In this coupling scheme, the data mining system is smoothly integrated into the \ndatabase or data warehouse system. The data mining subsystem is treated as one functional \ncomponent of an information system. \n \n \n \n \n\n \n \n \nMajor Issues in DataMining \n Data mining is not an easy task, as"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "e or data warehouse system. The data mining subsystem is treated as one functional \ncomponent of an information system. \n \n \n \n \n\n \n \n \nMajor Issues in DataMining \n Data mining is not an easy task, as the algorithms used can get very complex and data is not \nalways available at one place. It needs to be integrated from various heterogeneous data sources. \nThese factors also create some issues. \n \n Mining Methodology \nIn addition, mining methodologies should consider issues such as data uncertainty, noise, and \nincompleteness. Some mining methods explore how user specified measures can be used to \nassess the interestingness of discovered patterns as well as guide the discovery process.  \nMining various and new kinds of knowledge: Data mining covers a wide spectrum of data \nanalysis and knowledge discovery tasks, from data characterization and discrimination to \nassociation and correlation analysis, classification, regression, clustering, outlier analysis, \nsequence analysis, and trend "
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "wledge discovery tasks, from data characterization and discrimination to \nassociation and correlation analysis, classification, regression, clustering, outlier analysis, \nsequence analysis, and trend and evolution analysis. These tasks may use the same database in \ndifferent ways and require the development of numerous data mining techniques. Due to the \ndiversity of applications, new mining tasks continue to emerge, making data mining a dynamic \nand fast-growing field. For example, for effective knowledge discovery in information networks, \nintegrated clustering and ranking may lead to the discovery of high-quality clusters and object \nranks in large networks. \nMining knowledge in multidimensional space: When searching for knowledge in large data \nsets, we can explore the data in multidimensional space. That is, we can search for interesting \npatterns among combinations of dimensions (attributes) at varying levels of abstraction. Such \nmining is known as (exploratory) multidimensional"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "ensional space. That is, we can search for interesting \npatterns among combinations of dimensions (attributes) at varying levels of abstraction. Such \nmining is known as (exploratory) multidimensional data mining. In many cases, data can be \naggregated or viewed as a multidimensional data cube. Mining knowledge in cube space can \nsubstantially enhance the power and \nflexibility of data mining. \nData mining—an interdisciplinary effort: The power of data mining can be substantially \nenhanced by integrating new methods from multiple disciplines. For example, to mine data with \nnatural language text, it makes sense to fuse data mining methods with methods of information \nretrieval and natural language processing. As another example, consider the mining of software \nbugs in large programs. This form of mining, known as bug mining, benefits from the \nincorporation of software engineering knowledge into the data mining process. \nBoosting the power of discovery in a networked environment: Most"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": " form of mining, known as bug mining, benefits from the \nincorporation of software engineering knowledge into the data mining process. \nBoosting the power of discovery in a networked environment: Most data objects reside in a \nlinked or interconnected environment, whether it be the Web, database relations, files, or \n\n \n \ndocuments. Semantic links across multiple data objects can be used to advantage in data mining. \nKnowledge derived in one set of objects can be used to boost the discovery of knowledge in a \n“related” or semantically linked set of objects. \nHandling uncertainty, noise, or incompleteness of data: Data often contain noise, errors, \nexceptions, or uncertainty, or are incomplete. Errors and noise may confuse the data mining \nprocess, leading to the derivation of erroneous patterns. Data cleaning, data pre-processing, \noutlier detection and removal, and uncertainty reasoning are examples of techniques that need to \nbe integrated with the data mining process. \nPattern evalu"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "terns. Data cleaning, data pre-processing, \noutlier detection and removal, and uncertainty reasoning are examples of techniques that need to \nbe integrated with the data mining process. \nPattern evaluation and pattern- or constraint-guided mining: Not all the patterns generated by \ndata mining processes are interesting. What makes a pattern interesting may vary from user to \nuser. Therefore, techniques are needed to assess the interestingness of discovered patterns based \non subjective measures. These estimate the value of patterns with respect to a given user class, \nbased on user beliefs or expectations. Moreover, by using interestingness measures or user-\nspecified constraints to guide the discovery process, we may generate more interesting patterns \nand reduce the search space. \n \n User Interaction \nThe user plays an important role in the data mining process. Interesting areas of research include \nhow to interact with a data mining system, how to incorporate a user’s background kn"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "er Interaction \nThe user plays an important role in the data mining process. Interesting areas of research include \nhow to interact with a data mining system, how to incorporate a user’s background knowledge in \nmining, and how to visualize and comprehend data mining results. \n \nInteractive mining: The data mining process should be highly interactive. Thus, it is important \nto build flexible user interfaces and an exploratory mining environment, facilitating the user’s \ninteraction with the system. A user may like to first sample a set of data, explore general \ncharacteristics of the data, and estimate potential mining results. Interactive mining should allow \nusers to dynamically change the focus of a search, to refine mining requests based on returned \nresults, and to drill, dice, and pivot through the data and knowledge space interactively, \ndynamically exploring “cube space” while mining. \n \nIncorporation of background knowledge: Background knowledge, constraints, rules, and other "
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "vot through the data and knowledge space interactively, \ndynamically exploring “cube space” while mining. \n \nIncorporation of background knowledge: Background knowledge, constraints, rules, and other \ninformation regarding the domain under study should be incorporated into the knowledge \ndiscovery process. Such knowledge can be used for pattern evaluation as well as to guide the \nsearch toward interesting patterns. \n\n \n \nAd hoc data mining and data mining query languages: Query languages (e.g., SQL) have \nplayed an important role in flexible searching because they allow users to pose ad hoc queries. \nSimilarly, high-level data mining query languages or other high-level flexible user interfaces will \ngive users the freedom to define ad hoc data mining tasks. This should facilitate specification of \nthe relevant sets of data for analysis, the domain knowledge, the kinds of knowledge to be \nmined, and the conditions and constraints to be enforced on the discovered patterns. \nOptimization "
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "tion of \nthe relevant sets of data for analysis, the domain knowledge, the kinds of knowledge to be \nmined, and the conditions and constraints to be enforced on the discovered patterns. \nOptimization of the processing of such flexible mining requests is another promising area of \nstudy. \n \nPresentation and visualization of data mining results: How can a data mining system present \ndata mining results, vividly and flexibly, so that the discovered knowledge can be easily \nunderstood and directly usable by humans? This is especially crucial if the data mining process is \ninteractive. It requires the system to adopt expressive knowledge representations, user-friendly \ninterfaces, and visualization techniques. \n \n Efficiency and Scalability \nEfficiency and scalability are always considered when comparing data mining algorithms.As \ndata amounts continue tomultiply, these two factors are especially critical. \n \nEfficiency and scalability of data mining algorithms: Data mining algorithms must"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "n comparing data mining algorithms.As \ndata amounts continue tomultiply, these two factors are especially critical. \n \nEfficiency and scalability of data mining algorithms: Data mining algorithms must be efficient \nand scalable in order to effectively extract information from huge amounts of data in many data \nrepositories or in dynamic data streams. In other words, the running time of a data mining \nalgorithm must be predictable, short, and acceptable by applications. Efficiency, scalability, \nperformance, optimization, and the ability to execute in real time are key criteria that drive the \ndevelopment of many new data mining algorithms. \n \nParallel, distributed, and incremental mining algorithms: The humongous size of many data \nsets, the wide distribution of data, and the computational complexity of some data mining \nmethods are factors that motivate the development of parallel and distributed data-intensive \nmining algorithms. Such algorithms first partition the data into “pieces."
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": " complexity of some data mining \nmethods are factors that motivate the development of parallel and distributed data-intensive \nmining algorithms. Such algorithms first partition the data into “pieces.” Each piece is \nprocessed, in parallel, by searching for patterns. The parallel processes may interact with one \nanother. The patterns from each partition are eventually merged. \n \n\n \n \nCloud computing and cluster computing, which use computers in a distributed and \ncollaborative way to tackle very large-scale computational tasks, are also active research themes \nin parallel data mining. In addition, the high cost of some data mining processes and the \nincremental nature of input promote incremental data mining, which incorporates new data \nupdates without having to mine the entire data “from scratch.” Such methods perform knowledge \nmodification incrementally to amend and strengthen what was previously discovered. \n Diversity of Database Types \nThe wide diversity of database types bring"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": " scratch.” Such methods perform knowledge \nmodification incrementally to amend and strengthen what was previously discovered. \n Diversity of Database Types \nThe wide diversity of database types brings about challenges to data mining. These include \n \nHandling complex types of data: Diverse applications generate a wide spectrum of new data \ntypes, from structured data such as relational and data warehouse data to semi-structured and \nunstructured data; from stable data repositories to dynamic data streams; from simple data \nobjects to temporal data, biological sequences, sensor data, spatial data, hypertext data, \nmultimedia data, software program code,Web data, and social network data. It is unrealistic to \nexpect one data mining system to mine all kinds of data, given the diversity of data types and the \ndifferent goals of data mining. Domain- or application-dedicated data mining systems are being \nconstructed for in depth mining of specific kinds of data. The construction of effecti"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "ta types and the \ndifferent goals of data mining. Domain- or application-dedicated data mining systems are being \nconstructed for in depth mining of specific kinds of data. The construction of effective and \nefficient data mining tools for diverse applications remains a challenging and active area of \nresearch. \nMining dynamic, networked, and global data repositories: Multiple sources of data are \nconnected by the Internet and various kinds of networks, forming gigantic, distributed, and \nheterogeneous global information systems and networks. The discovery of knowledge from \ndifferent sources of structured, semi-structured, or unstructured yet interconnected data with \ndiverse data semantics poses great challenges to data mining. Mining such gigantic, \ninterconnected information networks may help disclose many more patterns and knowledge in \nheterogeneous data sets than can be discovered from a small set of isolated data repositories. \nWeb mining, multisource data mining, and informati"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "ay help disclose many more patterns and knowledge in \nheterogeneous data sets than can be discovered from a small set of isolated data repositories. \nWeb mining, multisource data mining, and information network mining have become \nchallenging and fast-evolving data mining fields. \n Data Mining and Society \nSocial impacts of data mining: With data mining penetrating our everyday lives, it is important \nto study the impact of data mining on society.How can we use data mining technology to benefit \nsociety? How can we guard against its misuse? The improper disclosure or use of data and the \n\n \n \npotential violation of individual privacy and data protection rights are areas of concern that need \nto be addressed. \nPrivacy-preserving data mining: Data mining will help scientific discovery, business \nmanagement, economy recovery, and security protection (e.g., the real-time discovery of \nintruders and cyberattacks). However, it poses the risk of disclosing an individual’s personal \ninformati"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "ness \nmanagement, economy recovery, and security protection (e.g., the real-time discovery of \nintruders and cyberattacks). However, it poses the risk of disclosing an individual’s personal \ninformation. Studies on privacy-preserving data publishing and data mining are ongoing. The \nphilosophy is to observe data sensitivity and preserve people’s privacy while performing \nsuccessful data mining. \nInvisible data mining: We cannot expect everyone in society to learn and master data mining \ntechniques. More and more systems should have data mining functions built within so that \npeople can perform data mining or use data mining results simply by mouse clicking, without \nany knowledge of data mining algorithms. Intelligent search engines and Internet-based stores \nperform such invisible data mining by incorporating data mining into their components to \nimprove their functionality and performance. This is done often unbeknownst to the user. For \nexample, when purchasing items online, users m"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "ning by incorporating data mining into their components to \nimprove their functionality and performance. This is done often unbeknownst to the user. For \nexample, when purchasing items online, users may be unaware that the store is likely collecting \ndata on the buying patterns of its customers, which may be used to recommend other items for \npurchase in the future. \nData Preprocessing \nData pre-processing is the process of transforming raw data into an understandable format. It is \nalso an important step in data mining as we cannot work with raw data. The quality of the data \nshould be checked before applying machine learning or data mining algorithms.Preprocessing of \ndata is mainly to check the data quality. The quality can be checked by the following: \n Accuracy: To check whether the data entered is correct or not. \n Completeness: To check whether the data is available or not recorded. \n Consistency: To check whether the same data is kept in all the places that do or do not matc"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "data entered is correct or not. \n Completeness: To check whether the data is available or not recorded. \n Consistency: To check whether the same data is kept in all the places that do or do not match. \n Timeliness: The data should be updated correctly. \n Believability: The data should be trustable. \n Interpretability: The understandability of the data. \nThere are 4 major tasks in data preprocessing – Data cleaning, Data integration, Data reduction, \nand Data transformation. \n\n \n \nData Cleaning \nData cleaning is the process of removing incorrect data, incomplete data, and inaccurate data \nfrom the datasets, and it also replaces the missing values. Here are some techniques for data \ncleaning: \nHandling Missing Values \n \nStandard values like “Not Available” or “NA” can be used to replace the missing values. \n \nMissing values can also be filled manually, but it is not recommended when that dataset is big. \n \nThe attribute’s mean value can be used to replace the missing value when t"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "e the missing values. \n \nMissing values can also be filled manually, but it is not recommended when that dataset is big. \n \nThe attribute’s mean value can be used to replace the missing value when the data is normally \ndistributed wherein in the case of non-normal distribution median value of the attribute can be \nused. \n \nWhile using regression or decision tree algorithms, the missing value can be replaced by the \nmost probable value. \nHandling Noisy Data \nNoisy generally means random error or containing unnecessary data points. Handling noisy data \nis one of the most important steps as it leads to the optimization of the model we are using Here \nare some of the methods to handle noisy data. \n \nBinning: This method is to smooth or handle noisy data. First, the data is sorted then, and then \nthe sorted values are separated and stored in the form of bins. There are three methods for \nsmoothing data in the bin. Smoothing by bin mean method: In this method, the values in the \nbin are "
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": " then \nthe sorted values are separated and stored in the form of bins. There are three methods for \nsmoothing data in the bin. Smoothing by bin mean method: In this method, the values in the \nbin are replaced by the mean value of the bin; Smoothing by bin median: In this method, the \nvalues in the bin are replaced by the median value; Smoothing by bin boundary: In this \nmethod, the using minimum and maximum values of the bin values are taken, and the closest \nboundary value replaces the values. \n \nRegression: This is used to smooth the data and will help to handle data when unnecessary data \nis present. For the analysis, purpose regression helps to decide the variable which is suitable for \nour analysis. \n\n \n \n \nClustering: This is used for finding the outliers and also in grouping the data. Clustering is \ngenerally used in unsupervised learning. \nData Integration \nThe process of combining multiple sources into a single dataset. The Data integration process is \none of the main compon"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "ata. Clustering is \ngenerally used in unsupervised learning. \nData Integration \nThe process of combining multiple sources into a single dataset. The Data integration process is \none of the main components of data management. There are some problems to be considered \nduring data integration. \n Schema integration: Integrates metadata(a set of data that describes other data) from different \nsources. \n Entity identification problem: Identifying entities from multiple databases. For example, the \nsystem or the user should know the student id of one database and studentname of another \ndatabase belonging to the same entity. \n Detecting and resolving data value concepts: The data taken from different databases while \nmerging may differ. The attribute values from one database may differ from another database. \nFor example, the date format may differ, like “MM/DD/YYYY” or “DD/MM/YYYY”. \nData Reduction \nThis process helps in the reduction of the volume of the data, which makes the analysis ea"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "other database. \nFor example, the date format may differ, like “MM/DD/YYYY” or “DD/MM/YYYY”. \nData Reduction \nThis process helps in the reduction of the volume of the data, which makes the analysis easier yet \nproduces the same or almost the same result. This reduction also helps to reduce storage space. \nSome of the data reduction techniques are dimensionality reduction, numerosity reduction, and \ndata compression. \n \nDimensionality reduction: This process is necessary for real-world applications as the data size \nis big. In this process, the reduction of random variables or attributes is done so that the \ndimensionality of the data set can be reduced. Combining and merging the attributes of the data \nwithout losing its original characteristics. This also helps in the reduction of storage space, and \ncomputation time is reduced. When the data is highly dimensional, a problem called the “Curse \nof Dimensionality” occurs. \n\n \n \n \nNumerosity Reduction: In this method, the representatio"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "e space, and \ncomputation time is reduced. When the data is highly dimensional, a problem called the “Curse \nof Dimensionality” occurs. \n\n \n \n \nNumerosity Reduction: In this method, the representation of the data is made smaller by \nreducing the volume. There will not be any loss of data in this reduction. \n \nData compression: The compressed form of data is called data compression. This compression \ncan be lossless or lossy. When there is no loss of information during compression, it is called \nlossless compression. Whereas lossy compression reduces information, but it removes only the \nunnecessary information. \n \nData Transformation \nThe change made in the format or the structure of the data is called data transformation. This \nstep can be simple or complex based on the requirements. There are some methods for data \ntransformation. \n \nSmoothing: With the help of algorithms, we can remove noise from the dataset, which helps in \nknowing the important features of the dataset. By smoot"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "ere are some methods for data \ntransformation. \n \nSmoothing: With the help of algorithms, we can remove noise from the dataset, which helps in \nknowing the important features of the dataset. By smoothing, we can find even a simple change \nthat helps in prediction. \n \nAggregation: In this method, the data is stored and presented in the form of a summary. The data \nset, which is from multiple sources, is integrated into with data analysis description. This is an \nimportant step since the accuracy of the data depends on the quantity and quality of the data. \nWhen the quality and the quantity of the data are good, the results are more relevant. \n \nDiscretization: The continuous data here is split into intervals. Discretization reduces the data \nsize. For example, rather than specifying the class time, we can set an interval like (3 pm-5 pm, \nor 6 pm-8 pm). \n \nNormalization: It is the method of scaling the data so that it can be represented in a smaller \nrange. Example ranging from -1.0"
  },
  {
    "source": "DataMining-Unit-1.pdf",
    "text": "class time, we can set an interval like (3 pm-5 pm, \nor 6 pm-8 pm). \n \nNormalization: It is the method of scaling the data so that it can be represented in a smaller \nrange. Example ranging from -1.0 to 1.0. \n Note : Refer to Unit-I PPT for Preprocessing Examples \n"
  }
]